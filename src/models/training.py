
"""
INTJæ¨¡å‹è®­ç»ƒç³»ç»Ÿ - ç¨³å¥ã€å¯å¤ç°ã€è‡ªåŠ¨åŒ–çš„æ¨¡å‹è®­ç»ƒæ¡†æ¶
åŸºäº30å¤©Kaggleç«èµ›ç»éªŒçš„æœ€ä½³å®è·µ
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, log_loss
from sklearn.base import BaseEstimator, clone
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
import warnings
warnings.filterwarnings('ignore')

class INTJModelFactory:
    """æ¨¡å‹å·¥å‚ - åˆ›å»ºå’Œç®¡ç†å„ç§æœºå™¨å­¦ä¹ æ¨¡å‹"""
    
    @staticmethod
    def create_lightgbm(params: Optional[Dict] = None) -> lgb.LGBMClassifier:
        """åˆ›å»ºLightGBMæ¨¡å‹"""
        default_params = {
            'n_estimators': 150,
            'learning_rate': 0.05,
            'num_leaves': 31,
            'max_depth': -1,
            'min_child_samples': 20,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'scale_pos_weight': 1.3,
            'random_state': 42,
            'n_jobs': -1,
            'verbose': -1
        }
        
        if params:
            default_params.update(params)
        
        return lgb.LGBMClassifier(**default_params)
    
    @staticmethod
    def create_xgboost(params: Optional[Dict] = None) -> xgb.XGBClassifier:
        """åˆ›å»ºXGBoostæ¨¡å‹"""
        default_params = {
            'n_estimators': 150,
            'learning_rate': 0.05,
            'max_depth': 6,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 0.1,
            'reg_lambda': 1.0,
            'random_state': 42,
            'n_jobs': -1,
            'verbosity': 0
        }
        
        if params:
            default_params.update(params)
        
        return xgb.XGBClassifier(**default_params)
    
    @staticmethod
    def create_catboost(params: Optional[Dict] = None) -> CatBoostClassifier:
        """åˆ›å»ºCatBoostæ¨¡å‹"""
        default_params = {
            'iterations': 150,
            'learning_rate': 0.05,
            'depth': 6,
            'l2_leaf_reg': 3.0,
            'random_seed': 42,
            'verbose': False,
            'thread_count': -1
        }
        
        if params:
            default_params.update(params)
        
        return CatBoostClassifier(**default_params)
    
    @staticmethod
    def create_random_forest(params: Optional[Dict] = None):
        """åˆ›å»ºéšæœºæ£®æ—æ¨¡å‹"""
        from sklearn.ensemble import RandomForestClassifier
        
        default_params = {
            'n_estimators': 100,
            'max_depth': None,
            'min_samples_split': 2,
            'min_samples_leaf': 1,
            'random_state': 42,
            'n_jobs': -1
        }
        
        if params:
            default_params.update(params)
        
        return RandomForestClassifier(**default_params)
    
    @staticmethod
    def create_model(model_type: str, params: Optional[Dict] = None):
        """é€šç”¨æ¨¡å‹åˆ›å»ºå‡½æ•°"""
        model_creators = {
            'lightgbm': INTJModelFactory.create_lightgbm,
            'xgboost': INTJModelFactory.create_xgboost,
            'catboost': INTJModelFactory.create_catboost,
            'random_forest': INTJModelFactory.create_random_forest
        }
        
        if model_type not in model_creators:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}ã€‚æ”¯æŒçš„ç±»å‹: {list(model_creators.keys())}")
        
        return model_creators[model_type](params)

class INTJCrossValidator:
    """äº¤å‰éªŒè¯å™¨ - ç¨³å¥çš„æ¨¡å‹è¯„ä¼°"""
    
    def __init__(self, 
                 n_splits: int = 5,
                 stratified: bool = True,
                 shuffle: bool = True,
                 random_state: int = 42):
        """
        åˆå§‹åŒ–äº¤å‰éªŒè¯å™¨
        
        å‚æ•°:
            n_splits: äº¤å‰éªŒè¯æŠ˜æ•°
            stratified: æ˜¯å¦ä½¿ç”¨åˆ†å±‚äº¤å‰éªŒè¯
            shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
            random_state: éšæœºç§å­
        """
        self.n_splits = n_splits
        self.stratified = stratified
        self.shuffle = shuffle
        self.random_state = random_state
        
        # åˆ›å»ºäº¤å‰éªŒè¯å™¨
        if stratified:
            self.cv = StratifiedKFold(
                n_splits=n_splits,
                shuffle=shuffle,
                random_state=random_state
            )
        else:
            self.cv = KFold(
                n_splits=n_splits,
                shuffle=shuffle,
                random_state=random_state
            )
    
    def cross_validate(self,
                      model,
                      X: pd.DataFrame,
                      y: pd.Series,
                      metrics: List[str] = ['accuracy', 'roc_auc'],
                      return_models: bool = False,
                      verbose: bool = True) -> Dict:
        """
        æ‰§è¡Œäº¤å‰éªŒè¯
        
        è¿”å›:
            åŒ…å«éªŒè¯ç»“æœçš„å­—å…¸
        """
        if verbose:
            print(f"ğŸ” å¼€å§‹{self.n_splits}æŠ˜äº¤å‰éªŒè¯...")
        
        # åˆå§‹åŒ–å­˜å‚¨
        fold_results = []
        oof_predictions = np.zeros(len(X))
        oof_probas = np.zeros(len(X))
        
        # å¦‚æœreturn_modelsä¸ºTrueï¼Œå­˜å‚¨æ¨¡å‹
        trained_models = [] if return_models else None
        
        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X, y)):
            if verbose:
                print(f"  æŠ˜å  {fold+1}/{self.n_splits}")
            
            # åˆ†å‰²æ•°æ®
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # è®­ç»ƒæ¨¡å‹
            fold_model = clone(model)
            fold_model.fit(X_train, y_train)
            
            # é¢„æµ‹
            if hasattr(fold_model, 'predict_proba'):
                val_probas = fold_model.predict_proba(X_val)[:, 1]
                val_preds = (val_probas > 0.5).astype(int)
                oof_probas[val_idx] = val_probas
            else:
                val_preds = fold_model.predict(X_val)
                val_probas = None
            
            oof_predictions[val_idx] = val_preds
            
            # è®¡ç®—æŒ‡æ ‡
            fold_metrics = self._calculate_metrics(y_val, val_preds, val_probas, metrics)
            fold_results.append(fold_metrics)
            
            # å­˜å‚¨æ¨¡å‹
            if return_models:
                trained_models.append(fold_model)
            
            if verbose:
                metric_str = ' | '.join([f'{k}: {v:.4f}' for k, v in fold_metrics.items()])
                print(f"    ç»“æœ: {metric_str}")
        
        # æ±‡æ€»ç»“æœ
        summary = self._summarize_results(fold_results, metrics)
        summary['oof_predictions'] = oof_predictions
        summary['oof_probas'] = oof_probas
        
        # è®¡ç®—OOFæŒ‡æ ‡
        oof_metrics = self._calculate_metrics(y, oof_predictions, oof_probas, metrics)
        summary['oof_metrics'] = oof_metrics
        
        if verbose:
            print(f"âœ… äº¤å‰éªŒè¯å®Œæˆ")
            print(f"ğŸ“Š å¹³å‡ç»“æœ: {summary['mean']}")
            print(f"ğŸ“ˆ OOFç»“æœ: {oof_metrics}")
        
        if return_models:
            summary['models'] = trained_models
        
        return summary
    
    def _calculate_metrics(self,
                          y_true: np.ndarray,
                          y_pred: np.ndarray,
                          y_proba: Optional[np.ndarray],
                          metrics: List[str]) -> Dict:
        """è®¡ç®—æŒ‡æ ‡"""
        results = {}
        
        for metric in metrics:
            if metric == 'accuracy':
                results[metric] = accuracy_score(y_true, y_pred)
            elif metric == 'roc_auc' and y_proba is not None:
                results[metric] = roc_auc_score(y_true, y_proba)
            elif metric == 'f1':
                results[metric] = f1_score(y_true, y_pred)
            elif metric == 'log_loss' and y_proba is not None:
                results[metric] = log_loss(y_true, y_proba)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æŒ‡æ ‡: {metric}")
        
        return results
    
    def _summarize_results(self, fold_results: List[Dict], metrics: List[str]) -> Dict:
        """æ±‡æ€»ç»“æœ"""
        summary = {
            'fold_results': fold_results,
            'mean': {},
            'std': {},
            'min': {},
            'max': {}
        }
        
        for metric in metrics:
            values = [result[metric] for result in fold_results]
            summary['mean'][metric] = np.mean(values)
            summary['std'][metric] = np.std(values)
            summary['min'][metric] = np.min(values)
            summary['max'][metric] = np.max(values)
        
        return summary

class INTJThresholdOptimizer:
    """é˜ˆå€¼ä¼˜åŒ–å™¨ - åŸºäºé¢„æµ‹æ¦‚ç‡ä¼˜åŒ–åˆ†ç±»é˜ˆå€¼"""
    
    def __init__(self, 
                 metric: str = 'f1',
                 threshold_range: Tuple[float, float] = (0.3, 0.7),
                 num_points: int = 50):
        """
        åˆå§‹åŒ–é˜ˆå€¼ä¼˜åŒ–å™¨
        
        å‚æ•°:
            metric: ä¼˜åŒ–æŒ‡æ ‡ ('f1', 'accuracy', 'custom')
            threshold_range: é˜ˆå€¼æœç´¢èŒƒå›´
            num_points: æœç´¢ç‚¹æ•°
        """
        self.metric = metric
        self.threshold_range = threshold_range
        self.num_points = num_points
        
        # å­˜å‚¨ç»“æœ
        self.results_ = None
        self.best_threshold_ = None
        self.best_score_ = None
    
    def optimize(self,
                 y_true: np.ndarray,
                 y_proba: np.ndarray,
                 custom_metric: Optional[Callable] = None) -> Dict:
        """
        ä¼˜åŒ–é˜ˆå€¼
        
        è¿”å›:
            ä¼˜åŒ–ç»“æœå­—å…¸
        """
        thresholds = np.linspace(self.threshold_range[0], 
                                self.threshold_range[1], 
                                self.num_points)
        
        results = []
        
        for threshold in thresholds:
            y_pred = (y_proba > threshold).astype(int)
            
            if self.metric == 'custom' and custom_metric:
                score = custom_metric(y_true, y_pred, y_proba)
            elif self.metric == 'f1':
                score = f1_score(y_true, y_pred)
            elif self.metric == 'accuracy':
                score = accuracy_score(y_true, y_pred)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æŒ‡æ ‡: {self.metric}")
            
            results.append({
                'threshold': threshold,
                'score': score,
                'positive_rate': y_pred.mean()
            })
        
        # æ‰¾åˆ°æœ€ä½³é˜ˆå€¼
        results_df = pd.DataFrame(results)
        best_idx = results_df['score'].idxmax()
        best_result = results_df.loc[best_idx]
        
        self.results_ = results_df
        self.best_threshold_ = best_result['threshold']
        self.best_score_ = best_result['score']
        
        return {
            'best_threshold': self.best_threshold_,
            'best_score': self.best_score_,
            'positive_rate_at_best': best_result['positive_rate'],
            'all_results': results_df
        }
    
    def plot_optimization(self, figsize: Tuple[int, int] = (10, 6)):
        """ç»˜åˆ¶ä¼˜åŒ–æ›²çº¿"""
        import matplotlib.pyplot as plt
        
        if self.results_ is None:
            raise ValueError("è¯·å…ˆè¿è¡Œoptimize()æ–¹æ³•")
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize)
        
        # æŒ‡æ ‡ vs é˜ˆå€¼
        ax1.plot(self.results_['threshold'], self.results_['score'], 
                'b-', linewidth=2, label='Score')
        ax1.axvline(self.best_threshold_, color='r', linestyle='--', 
                   label=f'Best: {self.best_threshold_:.3f}')
        ax1.set_xlabel('Threshold')
        ax1.set_ylabel('Score')
        ax1.set_title(f'Threshold Optimization ({self.metric})')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æ­£ç±»æ¯”ä¾‹ vs é˜ˆå€¼
        ax2.plot(self.results_['threshold'], self.results_['positive_rate'],
                'g-', linewidth=2, label='Positive Rate')
        ax2.axvline(self.best_threshold_, color='r', linestyle='--')
        ax2.set_xlabel('Threshold')
        ax2.set_ylabel('Positive Rate')
        ax2.set_title('Positive Rate vs Threshold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        return fig

class INTJModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨ - å®Œæ•´çš„è®­ç»ƒæµç¨‹ç®¡ç†"""
    
    def __init__(self,
                 model_type: str = 'lightgbm',
                 model_params: Optional[Dict] = None,
                 cv_strategy: str = 'stratified',
                 n_folds: int = 5):
        """
        åˆå§‹åŒ–æ¨¡å‹è®­ç»ƒå™¨
        
        å‚æ•°:
            model_type: æ¨¡å‹ç±»å‹
            model_params: æ¨¡å‹å‚æ•°
            cv_strategy: äº¤å‰éªŒè¯ç­–ç•¥
            n_folds: äº¤å‰éªŒè¯æŠ˜æ•°
        """
        self.model_type = model_type
        self.model_params = model_params or {}
        self.cv_strategy = cv_strategy
        self.n_folds = n_folds
        
        # åˆ›å»ºæ¨¡å‹
        self.model = INTJModelFactory.create_model(model_type, model_params)
        
        # åˆ›å»ºäº¤å‰éªŒè¯å™¨
        self.cv = INTJCrossValidator(
            n_splits=n_folds,
            stratified=(cv_strategy == 'stratified')
        )
        
        # å­˜å‚¨ç»“æœ
        self.cv_results_ = None
        self.final_model_ = None
        self.feature_importance_ = None
    
    def train(self,
              X_train: pd.DataFrame,
              y_train: pd.Series,
              X_val: Optional[pd.DataFrame] = None,
              y_val: Optional[pd.Series] = None,
              optimize_threshold: bool = False,
              verbose: bool = True) -> Dict:
        """
        è®­ç»ƒæ¨¡å‹
        
        è¿”å›:
            è®­ç»ƒç»“æœå­—å…¸
        """
        if verbose:
            print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {self.model_type} æ¨¡å‹")
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {X_train.shape}")
        
        # 1. äº¤å‰éªŒè¯
        if verbose:
            print("ğŸ“ˆ æ‰§è¡Œäº¤å‰éªŒè¯...")
        
        self.cv_results_ = self.cv.cross_validate(
            model=self.model,
            X=X_train,
            y=y_train,
            return_models=True,
            verbose=verbose
        )
        
        # 2. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        if verbose:
            print("ğŸ”§ è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        
        self.final_model_ = clone(self.model)
        self.final_model_.fit(X_train, y_train)
        
        # 3. ç‰¹å¾é‡è¦æ€§
        if hasattr(self.final_model_, 'feature_importances_'):
            self.feature_importance_ = pd.DataFrame({
                'feature': X_train.columns,
                'importance': self.final_model_.feature_importances_
            }).sort_values('importance', ascending=False)
        
        # 4. é˜ˆå€¼ä¼˜åŒ–
        threshold_result = None
        if optimize_threshold and self.cv_results_['oof_probas'] is not None:
            if verbose:
                print("ğŸ¯ ä¼˜åŒ–åˆ†ç±»é˜ˆå€¼...")
            
            optimizer = INTJThresholdOptimizer()
            threshold_result = optimizer.optimize(y_train, self.cv_results_['oof_probas'])
        
        # 5. éªŒè¯é›†è¯„ä¼°ï¼ˆå¦‚æœæœ‰ï¼‰
        val_metrics = None
        if X_val is not None and y_val is not None:
            if verbose:
                print("ğŸ“‹ éªŒè¯é›†è¯„ä¼°...")
            
            if hasattr(self.final_model_, 'predict_proba'):
                val_probas = self.final_model_.predict_proba(X_val)[:, 1]
                
                # ä½¿ç”¨æœ€ä½³é˜ˆå€¼æˆ–é»˜è®¤é˜ˆå€¼
                if threshold_result:
                    best_threshold = threshold_result['best_threshold']
                    val_preds = (val_probas > best_threshold).astype(int)
                else:
                    val_preds = (val_probas > 0.5).astype(int)
            else:
                val_preds = self.final_model_.predict(X_val)
                val_probas = None
            
            # è®¡ç®—æŒ‡æ ‡
            val_metrics = {
                'accuracy': accuracy_score(y_val, val_preds)
            }
            
            if val_probas is not None:
                val_metrics['roc_auc'] = roc_auc_score(y_val, val_probas)
                val_metrics['log_loss'] = log_loss(y_val, val_probas)
        
        # æ±‡æ€»ç»“æœ
        results = {
            'model_type': self.model_type,
            'cv_summary': self.cv_results_['mean'],
            'cv_std': self.cv_results_['std'],
            'oof_metrics': self.cv_results_['oof_metrics'],
            'final_model': self.final_model_,
            'feature_importance': self.feature_importance_,
            'threshold_optimization': threshold_result,
            'validation_metrics': val_metrics,
            'training_complete': True
        }
        
        if verbose:
            print("âœ… è®­ç»ƒå®Œæˆ")
            print(f"ğŸ“Š CVå¹³å‡å‡†ç¡®ç‡: {results['cv_summary'].get('accuracy', 0):.4f}")
            print(f"ğŸ“Š OOFå‡†ç¡®ç‡: {results['oof_metrics'].get('accuracy', 0):.4f}")
            
            if val_metrics:
                print(f"ğŸ“Š éªŒè¯é›†å‡†ç¡®ç‡: {val_metrics.get('accuracy', 0):.4f}")
        
        return results
    
    def predict(self, 
                X: pd.DataFrame, 
                threshold: Optional[float] = None,
                return_proba: bool = False):
        """
        ä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¿›è¡Œé¢„æµ‹
        
        å‚æ•°:
            X: ç‰¹å¾æ•°æ®
            threshold: åˆ†ç±»é˜ˆå€¼ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨0.5ï¼‰
            return_proba: æ˜¯å¦è¿”å›æ¦‚ç‡
        
        è¿”å›:
            é¢„æµ‹ç»“æœ
        """
        if self.final_model_ is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        
        if hasattr(self.final_model_, 'predict_proba'):
            probas = self.final_model_.predict_proba(X)
            
            if return_proba:
                return probas
            
            # åº”ç”¨é˜ˆå€¼
            if threshold is None:
                # ä½¿ç”¨è®­ç»ƒæ—¶ä¼˜åŒ–çš„é˜ˆå€¼
                if (self.cv_results_ and 
                    'threshold_optimization' in self.cv_results_ and 
                    self.cv_results_['threshold_optimization']):
                    threshold = self.cv_results_['threshold_optimization']['best_threshold']
                else:
                    threshold = 0.5
            
            return (probas[:, 1] > threshold).astype(int)
        else:
            return self.final_model_.predict(X)
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        import pickle
        
        if self.final_model_ is None:
            raise ValueError("æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥ä¿å­˜")
        
        model_data = {
            'model': self.final_model_,
            'model_type': self.model_type,
            'feature_importance': self.feature_importance_,
            'cv_results': self.cv_results_,
            'feature_names': list(self.final_model_.feature_names_in_) if hasattr(self.final_model_, 'feature_names_in_') else None
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    @classmethod
    def load_model(cls, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        import pickle
        
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        # åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹
        trainer = cls(
            model_type=model_data['model_type']
        )
        
        # æ¢å¤çŠ¶æ€
        trainer.final_model_ = model_data['model']
        trainer.feature_importance_ = model_data['feature_importance']
        trainer.cv_results_ = model_data['cv_results']
        
        return trainer

# æ¨¡å‹è¯„ä¼°å·¥å…·
class ModelEvaluationUtils:
    """æ¨¡å‹è¯„ä¼°å·¥å…·ç±»"""
    
    @staticmethod
    def create_model_comparison_report(models_results: List[Dict]) -> pd.DataFrame:
        """åˆ›å»ºæ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š"""
        comparison_data = []
        
        for result in models_results:
            comparison_data.append({
                'model_type': result.get('model_type', 'Unknown'),
                'cv_accuracy_mean': result.get('cv_summary', {}).get('accuracy', 0),
                'cv_accuracy_std': result.get('cv_std', {}).get('accuracy', 0),
                'oof_accuracy': result.get('oof_metrics', {}).get('accuracy', 0),
                'cv_roc_auc_mean': result.get('cv_summary', {}).get('roc_auc', 0),
                'cv_roc_auc_std': result.get('cv_std', {}).get('roc_auc', 0),
                'oof_roc_auc': result.get('oof_metrics', {}).get('roc_auc', 0),
                'training_time': result.get('training_time', 0)
            })
        
        df = pd.DataFrame(comparison_data)
        
        # æ’åº
        df = df.sort_values('oof_accuracy', ascending=False).reset_index(drop=True)
        
        return df
    
    @staticmethod
    def plot_model_comparison(comparison_df: pd.DataFrame, 
                             metric: str = 'oof_accuracy',
                             figsize: Tuple[int, int] = (10, 6)):
        """ç»˜åˆ¶æ¨¡å‹æ¯”è¾ƒå›¾"""
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        fig, ax = plt.subplots(figsize=figsize)
        
        # æ’åºæ•°æ®
        comparison_df = comparison_df.sort_values(metric, ascending=True)
        
        # åˆ›å»ºæ¡å½¢å›¾
        y_pos = np.arange(len(comparison_df))
        ax.barh(y_pos, comparison_df[metric], color='steelblue', alpha=0.8)
        
        # æ·»åŠ è¯¯å·®æ¡ï¼ˆå¦‚æœæœ‰ï¼‰
        if f'cv_{metric}_std' in comparison_df.columns:
            std_col = f'cv_{metric}_std'
            ax.errorbar(comparison_df[metric], y_pos, 
                       xerr=comparison_df[std_col], 
                       fmt='none', color='black', capsize=3)
        
        # è®¾ç½®æ ‡ç­¾
        ax.set_yticks(y_pos)
        ax.set_yticklabels(comparison_df['model_type'])
        ax.set_xlabel(metric.replace('_', ' ').title())
        ax.set_title(f'Model Comparison by {metric.replace("_", " ").title()}')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(comparison_df[metric]):
            ax.text(v + 0.01, i, f'{v:.4f}', va='center')
        
        plt.tight_layout()
        return fig

# å¯¼å‡ºä¸»è¦ç±»
__all__ = [
    'INTJModelFactory',
    'INTJCrossValidator',
    'INTJThresholdOptimizer',
    'INTJModelTrainer',
    'ModelEvaluationUtils'
]

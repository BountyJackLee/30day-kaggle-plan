
"""
Kaggleç«èµ›å·¥å…·åº“ - å®Œæ•´ç‰ˆ
åŒ…å«å®éªŒç®¡ç†ã€æ•°æ®å¤„ç†ã€å¯è§†åŒ–ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import pandas as pd
import numpy as np
import json
import pickle
import os
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Optional, Union
import warnings
warnings.filterwarnings('ignore')

class ExperimentLogger:
    """
    å®éªŒæ—¥å¿—è®°å½•å™¨ - ç”¨äºç³»ç»ŸåŒ–è·Ÿè¸ªæ‰€æœ‰å®éªŒ
    INTJé£æ ¼çš„å®Œæ•´å®éªŒç®¡ç†ç³»ç»Ÿ
    """
    
    def __init__(self, log_dir: str = "logs/experiments", project_name: str = "kaggle-project"):
        """
        åˆå§‹åŒ–å®éªŒæ—¥å¿—è®°å½•å™¨
        
        å‚æ•°:
            log_dir: æ—¥å¿—ç›®å½•
            project_name: é¡¹ç›®åç§°
        """
        self.log_dir = Path(log_dir)
        self.project_name = project_name
        self.experiment_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.current_experiment_dir = self.log_dir / self.experiment_id
        self.current_experiment_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—æ•°æ®ç»“æ„
        self.log_data = {
            "experiment_id": self.experiment_id,
            "project_name": project_name,
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "parameters": {},
            "metrics": {},
            "files": [],
            "status": "running",
            "notes": ""
        }
        
        print(f"ğŸ”¬ å®éªŒ {self.experiment_id} å·²å¯åŠ¨")
        print(f"ğŸ“ æ—¥å¿—ç›®å½•: {self.current_experiment_dir}")
    
    def log_parameters(self, params: Dict[str, Any]) -> None:
        """è®°å½•å®éªŒå‚æ•°"""
        self.log_data["parameters"].update(params)
        print(f"ğŸ“ è®°å½•å‚æ•°: {len(params)} ä¸ª")
    
    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -> None:
        """è®°å½•è¯„ä¼°æŒ‡æ ‡"""
        if step is not None:
            if "step_metrics" not in self.log_data:
                self.log_data["step_metrics"] = {}
            self.log_data["step_metrics"][step] = metrics
        else:
            self.log_data["metrics"].update(metrics)
        print(f"ğŸ“Š è®°å½•æŒ‡æ ‡: {metrics}")
    
    def log_file(self, file_path: str, description: str = "") -> None:
        """è®°å½•ç”Ÿæˆçš„æ–‡ä»¶"""
        file_info = {
            "path": file_path,
            "description": description,
            "timestamp": datetime.now().isoformat()
        }
        self.log_data["files"].append(file_info)
    
    def log_note(self, note: str) -> None:
        """è®°å½•å®éªŒç¬”è®°"""
        if "notes" not in self.log_data:
            self.log_data["notes"] = ""
        self.log_data["notes"] += f"[{datetime.now().strftime('%H:%M:%S')}] {note}\n"
    
    def save(self, status: str = "completed") -> None:
        """ä¿å­˜å®éªŒæ—¥å¿—"""
        self.log_data["end_time"] = datetime.now().isoformat()
        self.log_data["status"] = status
        
        # ä¿å­˜JSONæ—¥å¿—
        log_file = self.current_experiment_dir / "experiment_log.json"
        with open(log_file, 'w') as f:
            json.dump(self.log_data, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜äººç±»å¯è¯»ç‰ˆæœ¬
        txt_file = self.current_experiment_dir / "experiment_summary.txt"
        with open(txt_file, 'w') as f:
            f.write(self._generate_summary())
        
        print(f"ğŸ’¾ å®éªŒæ—¥å¿—å·²ä¿å­˜: {log_file}")
        print(f"ğŸ“‹ å®éªŒçŠ¶æ€: {status}")
    
    def _generate_summary(self) -> str:
        """ç”Ÿæˆå®éªŒæ‘˜è¦"""
        summary = f"""å®éªŒæ‘˜è¦æŠ¥å‘Š
{'='*60}
å®éªŒID: {self.log_data['experiment_id']}
é¡¹ç›®: {self.log_data['project_name']}
å¼€å§‹æ—¶é—´: {self.log_data['start_time']}
ç»“æŸæ—¶é—´: {self.log_data['end_time']}
çŠ¶æ€: {self.log_data['status']}
{'='*60}

ğŸ“Š å…³é”®æŒ‡æ ‡:
{self._format_metrics()}

âš™ï¸ å®éªŒå‚æ•°:
{self._format_parameters()}

ğŸ“ ç”Ÿæˆæ–‡ä»¶ ({len(self.log_data.get('files', []))}ä¸ª):
{self._format_files()}

ğŸ“ å®éªŒç¬”è®°:
{self.log_data.get('notes', 'æ— ')}
"""
        return summary
    
    def _format_metrics(self) -> str:
        """æ ¼å¼åŒ–æŒ‡æ ‡è¾“å‡º"""
        if not self.log_data.get("metrics"):
            return "  æ— æŒ‡æ ‡è®°å½•"
        
        metrics = self.log_data["metrics"]
        lines = []
        for key, value in metrics.items():
            if isinstance(value, float):
                lines.append(f"  {key}: {value:.6f}")
            else:
                lines.append(f"  {key}: {value}")
        return "\n".join(lines)
    
    def _format_parameters(self) -> str:
        """æ ¼å¼åŒ–å‚æ•°è¾“å‡º"""
        if not self.log_data.get("parameters"):
            return "  æ— å‚æ•°è®°å½•"
        
        params = self.log_data["parameters"]
        lines = []
        for key, value in params.items():
            if isinstance(value, dict):
                lines.append(f"  {key}:")
                for sub_key, sub_value in value.items():
                    lines.append(f"    {sub_key}: {sub_value}")
            else:
                lines.append(f"  {key}: {value}")
        return "\n".join(lines)
    
    def _format_files(self) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶åˆ—è¡¨"""
        if not self.log_data.get("files"):
            return "  æ— æ–‡ä»¶è®°å½•"
        
        files = self.log_data["files"]
        lines = []
        for i, file_info in enumerate(files, 1):
            lines.append(f"  {i}. {file_info['path']}")
            if file_info['description']:
                lines.append(f"     æè¿°: {file_info['description']}")
        return "\n".join(lines)

def reduce_memory_usage(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
    """
    ä¼˜åŒ–DataFrameå†…å­˜ä½¿ç”¨
    
    å‚æ•°:
        df: è¾“å…¥DataFrame
        verbose: æ˜¯å¦æ‰“å°ä¼˜åŒ–ä¿¡æ¯
    
    è¿”å›:
        ä¼˜åŒ–åçš„DataFrame
    """
    start_mem = df.memory_usage().sum() / 1024**2
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            # å¯¹è±¡ç±»å‹è½¬æ¢ä¸ºåˆ†ç±»
            df[col] = df[col].astype('category')
    
    end_mem = df.memory_usage().sum() / 1024**2
    
    if verbose:
        print(f"ğŸ“‰ å†…å­˜ä¼˜åŒ–: {start_mem:.2f} MB â†’ {end_mem:.2f} MB (å‡å°‘ {(start_mem-end_mem)/start_mem*100:.1f}%)")
    
    return df

def plot_feature_importance(model, feature_names, top_n: int = 20, figsize=(10, 8)):
    """
    ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾
    
    å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆæ”¯æŒLightGBMã€XGBoostã€RandomForestç­‰ï¼‰
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        top_n: æ˜¾ç¤ºå‰Nä¸ªé‡è¦ç‰¹å¾
        figsize: å›¾å½¢å¤§å°
    """
    plt.figure(figsize=figsize)
    
    # æ ¹æ®æ¨¡å‹ç±»å‹è·å–ç‰¹å¾é‡è¦æ€§
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    elif hasattr(model, 'feature_importance'):
        importances = model.feature_importance()
    elif hasattr(model, 'coef_'):
        importances = np.abs(model.coef_[0])
    else:
        raise ValueError("æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
    
    # åˆ›å»ºDataFrame
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False).head(top_n)
    
    # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
    bars = plt.barh(range(len(importance_df)), importance_df['importance'], align='center')
    plt.yticks(range(len(importance_df)), importance_df['feature'])
    plt.xlabel('ç‰¹å¾é‡è¦æ€§')
    plt.title(f'Top {top_n} ç‰¹å¾é‡è¦æ€§')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, imp) in enumerate(zip(bars, importance_df['importance'])):
        width = bar.get_width()
        plt.text(width * 1.01, bar.get_y() + bar.get_height()/2, 
                f'{imp:.4f}', va='center', fontsize=9)
    
    plt.tight_layout()
    return importance_df

def analyze_prediction_distribution(predictions: np.ndarray, 
                                   true_labels: Optional[np.ndarray] = None,
                                   thresholds: List[float] = None) -> Dict[str, Any]:
    """
    åˆ†æé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
    
    å‚æ•°:
        predictions: é¢„æµ‹æ¦‚ç‡æ•°ç»„ï¼ˆ0-1ä¹‹é—´ï¼‰
        true_labels: çœŸå®æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
        thresholds: è¦åˆ†æçš„é˜ˆå€¼åˆ—è¡¨
    
    è¿”å›:
        åˆ†å¸ƒåˆ†æå­—å…¸
    """
    if thresholds is None:
        thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]
    
    analysis = {
        "ç»Ÿè®¡é‡": {
            "å¹³å‡å€¼": float(predictions.mean()),
            "æ ‡å‡†å·®": float(predictions.std()),
            "æœ€å°å€¼": float(predictions.min()),
            "æœ€å¤§å€¼": float(predictions.max()),
            "ä¸­ä½æ•°": float(np.median(predictions)),
            "ååº¦": float(pd.Series(predictions).skew())
        },
        "åˆ†å¸ƒåˆ†ä½æ•°": {
            f"{p}åˆ†ä½": float(np.percentile(predictions, p)) 
            for p in [10, 25, 50, 75, 90]
        },
        "é˜ˆå€¼åˆ†æ": {},
        "é¢„æµ‹åˆ†ç±»": {}
    }
    
    # é˜ˆå€¼åˆ†æ
    for threshold in thresholds:
        binary_preds = (predictions > threshold).astype(int)
        analysis["é˜ˆå€¼åˆ†æ"][f"é˜ˆå€¼={threshold:.2f}"] = {
            "æ­£ç±»æ¯”ä¾‹": float(binary_preds.mean()),
            "æ­£ç±»æ•°é‡": int(binary_preds.sum()),
            "è´Ÿç±»æ•°é‡": int(len(binary_preds) - binary_preds.sum())
        }
    
    # é¢„æµ‹åˆ†ç±»ï¼ˆåŸºäºè‡ªç„¶é˜ˆå€¼0.5ï¼‰
    binary_preds = (predictions > 0.5).astype(int)
    analysis["é¢„æµ‹åˆ†ç±»"]["é˜ˆå€¼=0.50"] = {
        "æ­£ç±»æ¯”ä¾‹": float(binary_preds.mean()),
        "æ­£ç±»æ•°é‡": int(binary_preds.sum()),
        "è´Ÿç±»æ•°é‡": int(len(binary_preds) - binary_preds.sum())
    }
    
    # å¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼Œè®¡ç®—æ›´å¤šæŒ‡æ ‡
    if true_labels is not None:
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        analysis["æ€§èƒ½æŒ‡æ ‡"] = {}
        for threshold in thresholds:
            binary_preds = (predictions > threshold).astype(int)
            analysis["æ€§èƒ½æŒ‡æ ‡"][f"é˜ˆå€¼={threshold:.2f}"] = {
                "å‡†ç¡®ç‡": float(accuracy_score(true_labels, binary_preds)),
                "ç²¾ç¡®ç‡": float(precision_score(true_labels, binary_preds, zero_division=0)),
                "å¬å›ç‡": float(recall_score(true_labels, binary_preds, zero_division=0)),
                "F1åˆ†æ•°": float(f1_score(true_labels, binary_preds, zero_division=0))
            }
    
    return analysis

def save_submission(predictions: np.ndarray, 
                   sample_submission_path: str,
                   output_path: str,
                   threshold: float = 0.5,
                   competition_format: str = "binary") -> str:
    """
    ç”ŸæˆKaggleæäº¤æ–‡ä»¶
    
    å‚æ•°:
        predictions: é¢„æµ‹æ¦‚ç‡æˆ–æ ‡ç­¾
        sample_submission_path: ç¤ºä¾‹æäº¤æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        threshold: äºŒåˆ†ç±»é˜ˆå€¼
        competition_format: ç«èµ›æ ¼å¼ ('binary', 'probability', 'regression')
    
    è¿”å›:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    # è¯»å–ç¤ºä¾‹æäº¤æ–‡ä»¶
    sample_df = pd.read_csv(sample_submission_path)
    
    # æ ¹æ®æ ¼å¼å¤„ç†é¢„æµ‹
    if competition_format == "binary":
        # äºŒåˆ†ç±»ï¼šåº”ç”¨é˜ˆå€¼
        binary_predictions = (predictions > threshold).astype(int)
        sample_df.iloc[:, 1] = binary_predictions
    elif competition_format == "probability":
        # æ¦‚ç‡ï¼šç›´æ¥ä½¿ç”¨
        sample_df.iloc[:, 1] = predictions
    elif competition_format == "regression":
        # å›å½’ï¼šç›´æ¥ä½¿ç”¨
        sample_df.iloc[:, 1] = predictions
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç«èµ›æ ¼å¼: {competition_format}")
    
    # ä¿å­˜æ–‡ä»¶
    sample_df.to_csv(output_path, index=False)
    print(f"ğŸ’¾ æäº¤æ–‡ä»¶å·²ä¿å­˜: {output_path}")
    print(f"ğŸ“Š é¢„æµ‹ç»Ÿè®¡: å½¢çŠ¶={sample_df.shape}, æ­£ç±»æ¯”ä¾‹={sample_df.iloc[:, 1].mean():.3f}")
    
    return output_path

def create_cv_folds(df: pd.DataFrame, 
                   target: str,
                   n_splits: int = 5,
                   stratified: bool = True,
                   shuffle: bool = True,
                   random_state: int = 42) -> pd.DataFrame:
    """
    åˆ›å»ºäº¤å‰éªŒè¯æŠ˜å 
    
    å‚æ•°:
        df: è¾“å…¥DataFrame
        target: ç›®æ ‡åˆ—å
        n_splits: æŠ˜å æ•°é‡
        stratified: æ˜¯å¦åˆ†å±‚
        shuffle: æ˜¯å¦æ‰“ä¹±
        random_state: éšæœºç§å­
    
    è¿”å›:
        åŒ…å«foldåˆ—çš„DataFrame
    """
    from sklearn.model_selection import StratifiedKFold, KFold
    
    df_folds = df.copy()
    
    if stratified and target in df.columns:
        # åˆ†å±‚KæŠ˜
        skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)
        df_folds['fold'] = -1
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(df, df[target])):
            df_folds.loc[val_idx, 'fold'] = fold
    else:
        # æ™®é€šKæŠ˜
        kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)
        df_folds['fold'] = -1
        
        for fold, (train_idx, val_idx) in enumerate(kf.split(df)):
            df_folds.loc[val_idx, 'fold'] = fold
    
    print(f"âœ… åˆ›å»ºäº† {n_splits} æŠ˜äº¤å‰éªŒè¯")
    print(f"ğŸ“Š æ¯æŠ˜æ ·æœ¬æ•°: {df_folds['fold'].value_counts().sort_index().to_dict()}")
    
    return df_folds

def compare_feature_distributions(train_df: pd.DataFrame, 
                                 test_df: pd.DataFrame,
                                 features: List[str] = None,
                                 max_features: int = 20) -> pd.DataFrame:
    """
    æ¯”è¾ƒè®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾åˆ†å¸ƒ
    
    å‚æ•°:
        train_df: è®­ç»ƒé›†DataFrame
        test_df: æµ‹è¯•é›†DataFrame
        features: è¦æ¯”è¾ƒçš„ç‰¹å¾åˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºæ‰€æœ‰å…±åŒç‰¹å¾ï¼‰
        max_features: æœ€å¤šæ˜¾ç¤ºçš„ç‰¹å¾æ•°é‡
    
    è¿”å›:
        åˆ†å¸ƒæ¯”è¾ƒçš„DataFrame
    """
    if features is None:
        # è·å–å…±åŒç‰¹å¾
        common_features = list(set(train_df.columns) & set(test_df.columns))
    else:
        common_features = [f for f in features if f in train_df.columns and f in test_df.columns]
    
    # é™åˆ¶ç‰¹å¾æ•°é‡
    if len(common_features) > max_features:
        print(f"âš ï¸  ç‰¹å¾è¿‡å¤š ({len(common_features)})ï¼Œåªæ˜¾ç¤ºå‰{max_features}ä¸ª")
        common_features = common_features[:max_features]
    
    comparison_data = []
    
    for feature in common_features:
        train_vals = train_df[feature]
        test_vals = test_df[feature]
        
        # æ•°å€¼ç‰¹å¾
        if pd.api.types.is_numeric_dtype(train_vals):
            comparison = {
                'ç‰¹å¾': feature,
                'ç±»å‹': 'æ•°å€¼',
                'è®­ç»ƒé›†å‡å€¼': train_vals.mean(),
                'æµ‹è¯•é›†å‡å€¼': test_vals.mean(),
                'å‡å€¼å·®å¼‚%': abs((train_vals.mean() - test_vals.mean()) / train_vals.mean() * 100) if train_vals.mean() != 0 else float('inf'),
                'è®­ç»ƒé›†ç¼ºå¤±%': train_vals.isna().mean() * 100,
                'æµ‹è¯•é›†ç¼ºå¤±%': test_vals.isna().mean() * 100
            }
        else:
            # ç±»åˆ«ç‰¹å¾
            train_top = train_vals.mode().iloc[0] if not train_vals.mode().empty else None
            test_top = test_vals.mode().iloc[0] if not test_vals.mode().empty else None
            
            comparison = {
                'ç‰¹å¾': feature,
                'ç±»å‹': 'ç±»åˆ«',
                'è®­ç»ƒé›†ä¼—æ•°': train_top,
                'æµ‹è¯•é›†ä¼—æ•°': test_top,
                'ä¼—æ•°æ˜¯å¦ä¸€è‡´': train_top == test_top,
                'è®­ç»ƒé›†ç¼ºå¤±%': train_vals.isna().mean() * 100,
                'æµ‹è¯•é›†ç¼ºå¤±%': test_vals.isna().mean() * 100
            }
        
        comparison_data.append(comparison)
    
    comparison_df = pd.DataFrame(comparison_data)
    
    if not comparison_df.empty:
        print(f"ğŸ” ç‰¹å¾åˆ†å¸ƒæ¯”è¾ƒå®Œæˆï¼Œå…±æ¯”è¾ƒ {len(comparison_df)} ä¸ªç‰¹å¾")
        
        # è¯†åˆ«æ½œåœ¨é—®é¢˜
        numeric_df = comparison_df[comparison_df['ç±»å‹'] == 'æ•°å€¼']
        if not numeric_df.empty:
            problematic = numeric_df[numeric_df['å‡å€¼å·®å¼‚%'] > 20]
            if len(problematic) > 0:
                print(f"âš ï¸  å‘ç° {len(problematic)} ä¸ªæ•°å€¼ç‰¹å¾åˆ†å¸ƒå·®å¼‚ > 20%")
                print(problematic[['ç‰¹å¾', 'å‡å€¼å·®å¼‚%']].to_string())
    
    return comparison_df

def visualize_prediction_distribution(predictions: np.ndarray, 
                                     true_labels: Optional[np.ndarray] = None,
                                     figsize: tuple = (12, 8)):
    """
    å¯è§†åŒ–é¢„æµ‹åˆ†å¸ƒ
    
    å‚æ•°:
        predictions: é¢„æµ‹æ¦‚ç‡
        true_labels: çœŸå®æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
        figsize: å›¾å½¢å¤§å°
    """
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    
    # 1. é¢„æµ‹æ¦‚ç‡ç›´æ–¹å›¾
    axes[0, 0].hist(predictions, bins=50, alpha=0.7, color='blue', edgecolor='black')
    axes[0, 0].axvline(x=0.5, color='red', linestyle='--', label='é˜ˆå€¼=0.5')
    axes[0, 0].set_xlabel('é¢„æµ‹æ¦‚ç‡')
    axes[0, 0].set_ylabel('é¢‘æ•°')
    axes[0, 0].set_title('é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
    sorted_probs = np.sort(predictions)
    cum_probs = np.arange(1, len(sorted_probs)+1) / len(sorted_probs)
    axes[0, 1].plot(sorted_probs, cum_probs, color='green', linewidth=2)
    axes[0, 1].set_xlabel('é¢„æµ‹æ¦‚ç‡')
    axes[0, 1].set_ylabel('ç´¯ç§¯æ¯”ä¾‹')
    axes[0, 1].set_title('ç´¯ç§¯åˆ†å¸ƒå‡½æ•° (CDF)')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. ç®±çº¿å›¾
    axes[1, 0].boxplot(predictions, vert=False)
    axes[1, 0].set_xlabel('é¢„æµ‹æ¦‚ç‡')
    axes[1, 0].set_title('é¢„æµ‹æ¦‚ç‡ç®±çº¿å›¾')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. å¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼Œæ˜¾ç¤ºæ­£è´Ÿç±»åˆ†å¸ƒ
    if true_labels is not None:
        pos_probs = predictions[true_labels == 1]
        neg_probs = predictions[true_labels == 0]
        
        axes[1, 1].hist(pos_probs, bins=30, alpha=0.5, color='green', label='æ­£ç±»', density=True)
        axes[1, 1].hist(neg_probs, bins=30, alpha=0.5, color='red', label='è´Ÿç±»', density=True)
        axes[1, 1].axvline(x=0.5, color='black', linestyle='--', label='é˜ˆå€¼=0.5')
        axes[1, 1].set_xlabel('é¢„æµ‹æ¦‚ç‡')
        axes[1, 1].set_ylabel('å¯†åº¦')
        axes[1, 1].set_title('æ­£è´Ÿç±»é¢„æµ‹åˆ†å¸ƒ')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
    else:
        # å¦‚æœæ²¡æœ‰çœŸå®æ ‡ç­¾ï¼Œæ˜¾ç¤ºQQå›¾
        from scipy import stats
        stats.probplot(predictions, dist="norm", plot=axes[1, 1])
        axes[1, 1].set_title('QQå›¾ï¼ˆæ­£æ€æ€§æ£€éªŒï¼‰')
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

# å¯¼å‡ºæ‰€æœ‰å‡½æ•°
__all__ = [
    'ExperimentLogger',
    'reduce_memory_usage',
    'plot_feature_importance',
    'analyze_prediction_distribution',
    'save_submission',
    'create_cv_folds',
    'compare_feature_distributions',
    'visualize_prediction_distribution'
]
